# -*- coding: utf-8 -*-
"""Transformers_Keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZXl22Wou1PGqq7bEV3Qy-fWnMNeSjp9_
"""

# Libraries
from google.colab import drive
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, SimpleRNN, GRU
from tensorflow.keras.optimizers import RMSprop # Optimizer
from pickle import load
import math
from sklearn.metrics import mean_squared_error
import numpy as np
from numpy import array
from keras.layers import Bidirectional
from sklearn.preprocessing import StandardScaler
import torch.nn as nn
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import seaborn as sns
import yfinance as yf

# Load data and print examples
drive.mount('/content/drive')
Ruta_Archivo = '/content/drive/MyDrive/Ventas_Hora_V2.csv'

# Load data
Datos=pd.read_csv(Ruta_Archivo,header=0, sep=";", decimal=",")
Datos.head()

Data=pd.DataFrame(Datos['OT'])

Data=Datos['OT'].to_numpy()

Data=pd.DataFrame(Datos.iloc[:, 1])

Data

plt.figure(figsize=(9, 5))
plt.plot(Data,color='black')
plt.title('Serie temporal - Ventas')
plt.xlabel('Tiempo')
plt.ylabel('Cantidad de ventas')
plt.show()

# Escalar los datos
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(Data)

# Crear secuencias de datos para LSTM
def create_sequences(data, sequence_length):
    sequences = []
    labels = []
    for i in range(len(data) - sequence_length):
        sequences.append(data[i:i+sequence_length])
        labels.append(data[i+sequence_length])
    return np.array(sequences), np.array(labels)

sequence_length = 1  # Longitud de la secuencia
X, y = create_sequences(scaled_data, sequence_length)

X[0], y[0]

# Dividir en conjuntos de entrenamiento, validación y prueba
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)

# Añadir una dimensión adicional para el modelo
X_train = np.expand_dims(X_train, axis=-1)
X_val = np.expand_dims(X_val, axis=-1)
X_test = np.expand_dims(X_test, axis=-1)

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Normalización y capa de atención
    x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)
    x = Dropout(dropout)(x)
    res = x + inputs

    # Normalización y capa feed-forward
    x = LayerNormalization(epsilon=1e-6)(res)
    x = Dense(ff_dim, activation='relu')(x)
    x = Dropout(dropout)(x)
    x = Dense(inputs.shape[-1])(x)
    return x + res

def build_model(input_shape, head_size, num_heads, ff_dim, num_transformer_blocks, mlp_units, dropout=0, mlp_dropout=0):
    inputs = Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    x = GlobalAveragePooling1D()(x)
    for dim in mlp_units:
        x = Dense(dim, activation='relu')(x)
        x = Dropout(mlp_dropout)(x)
    outputs = Dense(1)(x)
    return Model(inputs, outputs)

input_shape = (sequence_length, 1)
model = build_model(
    input_shape,
    head_size=256 ,
    num_heads=4, # El número de cabezas de atención en el bloque del transformer. Múltiples cabezas permiten al modelo enfocarse en diferentes partes de la secuencia simultáneamente.
    ff_dim=4, # La dimensionalidad de la capa de feed-forward dentro del bloque del transformer. Después de la atención, los datos pasan por una red neuronal feed-forward de dos capas donde ff_dim
    #es el tamaño de la capa oculta.
    num_transformer_blocks=4, # El número de bloques de transformer en la arquitectura. Cada bloque consiste en una capa de atención multi-cabeza y una red feed-forward.
    mlp_units=[128], # neuronas en red multicapa
    dropout=0.1, # La tasa de abandono para las capas dentro del bloque del transformer. Esto ayuda a prevenir el sobreajuste aplicando regularización.
    mlp_dropout=0.1, # La tasa de abandono para las capas de MLP que siguen a los bloques del transformer. Esto también ayuda a prevenir el sobreajuste
)

model.compile(optimizer='adam', loss='mse')

history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=20,
    batch_size=32
)

test_loss = model.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss:.4f}')

predictions = model.predict(X_test)
predictions = scaler.inverse_transform(predictions)
y_test = scaler.inverse_transform(y_test)

plt.figure(figsize=(9, 5))
plt.plot(history.history['loss'], label='Entrenamiento - Loss')
plt.plot(history.history['val_loss'], label='Validación - Loss')
plt.title('Entrenamiento y Validación Loss')
plt.xlabel('Epocas')
plt.ylabel('Loss')
plt.legend()
plt.savefig('Overfitting_Trans.png')
plt.show()

plt.figure(figsize=(9, 5))
plt.plot(y_test, color='black', label='Datos observados')
plt.plot(predictions,color='red', label='Predicción')
plt.title('Predicción vs datos observados')
plt.xlabel('Tiempo')
plt.ylabel('Cantidad')
plt.legend()
plt.savefig('Ajuste_Trans.png')
plt.show()

# Suponiendo train_predictions y y_train son numpy arrays
def calculate_rmse(predictions, targets):
    mse = np.mean((predictions - targets) ** 2)
    rmse = np.sqrt(mse)
    return rmse

# Ejemplo de uso
RSME = calculate_rmse(predictions, y_test)
print(f'RMSE: {RSME:.2f}')

Media= np.mean(y_test)
Media

PorcentajeError= (RSME/Media)*100
PorcentajeError