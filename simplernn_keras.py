# -*- coding: utf-8 -*-
"""SimpleRNN_Keras.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A2QsA2YktdlLRphHacmh_0ZP3erm_hsw
"""

# Libraries
from google.colab import drive
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, SimpleRNN, GRU
from tensorflow.keras.optimizers import RMSprop # Optimizer
from pickle import load
import math
from sklearn.metrics import mean_squared_error
import numpy as np
from numpy import array
from keras.layers import Bidirectional
from sklearn.preprocessing import StandardScaler
import torch.nn as nn
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, LayerNormalization, Dropout, MultiHeadAttention, GlobalAveragePooling1D
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import seaborn as sns

# Load data and print examples
drive.mount('/content/drive')
Ruta_Archivo = '/content/drive/MyDrive/Ventas_Hora_V2.csv'

# Load data
Datos=pd.read_csv(Ruta_Archivo,header=0, sep=";", decimal=".")
Datos.head()

Data=pd.DataFrame(Datos.iloc[:, 1])

Data

plt.figure(figsize=(9, 5))
plt.plot(Data,color='black')
plt.title('Serie temporal - Ventas')
plt.xlabel('Cantidad de observaciones')
plt.ylabel('Cantidad de ventas')
plt.savefig('grafico.png')
plt.show()

# Escalar los datos
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(Data)

scaled_data[:5]

# Crear secuencias de datos
def create_sequences(data, sequence_length):
    sequences = []
    labels = []
    for i in range(len(data) - sequence_length):
        sequences.append(data[i:i+sequence_length])
        labels.append(data[i+sequence_length])
    return np.array(sequences), np.array(labels)

sequence_length = 1  # Longitud de la secuencia
X, y = create_sequences(scaled_data, sequence_length)

"""La longitud de la secuencia es muy importante para predecir. En el caso de las ventas por hora se usa 18, ya que cada 18 observaciones se cumple un ciclo."""

X[0], y[0]

"""Al crear secuencias entonces tengo menos observaciones porque agrupo en predictoras y predichas. Por eso len(Datos) es mayor, son los originales"""

len(X), len(y)

# Dividir en conjuntos de entrenamiento, validación y prueba
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, shuffle=False)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, shuffle=False)

"""Cuando creo los sets de entrenamiento, test y validación usó 70% para training, 15% para test y 15% para validación y uso train_test_split que proviene de sklearn."""

len(X_train), len(X_test), len(X_val)

len(X_train)+ len(X_test)+ len(X_val)

len(Data)

# Modelo vainilla ya que solo se tiene una sola capa densa

Modelo = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(50, activation='relu', return_sequences=False, input_shape=(sequence_length, 1)),
    tf.keras.layers.Dense(1)
])

"""Este es un modelo apilado o stack, tiene varias capas, en este caso 2. Para este problemática se prefiere un modelo sencillo con una sola capa oculta."""

Modelo = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(50, activation='relu', return_sequences=False, input_shape=(sequence_length, 1)),
    tf.keras.layers.Dense(50, activation='relu'),
    tf.keras.layers.Dense(1)
])

Modelo.compile(optimizer='adam', loss='mse')

history = Modelo.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))

test_loss = Modelo.evaluate(X_test, y_test)
print(f'Test Loss: {test_loss:.4f}')

predictions = Modelo.predict(X_test)
predictions = scaler.inverse_transform(predictions)
y_test = scaler.inverse_transform(y_test)

plt.figure(figsize=(9, 5))
plt.plot(history.history['loss'], label='Entrenamiento - Loss')
plt.plot(history.history['val_loss'], label='Validación - Loss')
plt.title('Entrenamiento y Validación Loss')
plt.xlabel('Epocas')
plt.ylabel('Loss')
plt.legend()
plt.savefig('Overfitting_RNN.png')
plt.show()

plt.figure(figsize=(9, 5))
plt.plot(y_test, color='black', label='Datos observados')
plt.plot(predictions,color='red', label='Predicción')
plt.title('Predicción vs datos observados')
plt.xlabel('Tiempo')
plt.ylabel('Cantidad')
plt.legend()
plt.savefig('Ajuste_RNN.png')
plt.show()

# Suponiendo train_predictions y y_train son numpy arrays
def calculate_rmse(predictions, targets):
    mse = np.mean((predictions - targets) ** 2)
    rmse = np.sqrt(mse)
    return rmse

# Ejemplo de uso
RSME = calculate_rmse(predictions, y_test)
print(f'RMSE: {RSME:.2f}')

Media= np.mean(y_test)
Media

PorcentajeError= (RSME/Media)*100
round(PorcentajeError,2)